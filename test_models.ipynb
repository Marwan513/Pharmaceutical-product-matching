{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isu2JZC8wnUx"
      },
      "source": [
        "### Importing Required Libraries\n",
        "\n",
        "This cell imports various Python libraries needed for data processing, machine learning, and text analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EmsrUyYZwnUz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from time import time\n",
        "from rapidfuzz import process, fuzz\n",
        "from deep_translator import GoogleTranslator\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import joblib\n",
        "import os\n",
        "import urllib.request\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTNmFHyTwnU0"
      },
      "source": [
        "### Loading the Trained Model and Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuglOwa3wnU0",
        "outputId": "e3ecb4bf-e301-4c4d-d075-340f55a9be4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models not found. Downloading...\n",
            "Download complete. Extracting...\n",
            "Extraction complete.\n",
            "Model and vectorizer loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# Define paths\n",
        "model_dir = \"models\"\n",
        "model_file = os.path.join(model_dir, \"logistic_regression_model.pkl\")\n",
        "vectorizer_file = os.path.join(model_dir, \"tfidf_vectorizer.pkl\")\n",
        "zip_url = \"https://github.com/Marwan513/Pharmaceutical-product-matching/releases/download/v1.0/models.zip\"\n",
        "zip_file = \"models.zip\"\n",
        "\n",
        "# Check if models exist, if not, download and extract\n",
        "if not (os.path.exists(model_file) and os.path.exists(vectorizer_file)):\n",
        "    print(\"Models not found. Downloading...\")\n",
        "    urllib.request.urlretrieve(zip_url, zip_file)\n",
        "    print(\"Download complete. Extracting...\")\n",
        "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "        zip_ref.extractall()\n",
        "    os.remove(zip_file)\n",
        "    print(\"Extraction complete.\")\n",
        "else:\n",
        "    print(\"Models already exist. Skipping download.\")\n",
        "\n",
        "# Load the model and vectorizer\n",
        "model = joblib.load(model_file)\n",
        "vectorizer = joblib.load(vectorizer_file)\n",
        "\n",
        "print(\"Model and vectorizer loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7vIBQmOwnU1"
      },
      "source": [
        "### Function to Preprocesses the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8FikT_8bwnU1"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(dataset, master_file):\n",
        "    \"\"\"\n",
        "    Preprocesses the dataset by translating English seller item names to Arabic and normalizing the Arabic text.\n",
        "\n",
        "    This function performs the following:\n",
        "    1. Translates English product names to Arabic using fuzzy matching with a master file and Google Translator as a fallback.\n",
        "    2. Normalizes the Arabic text by removing diacritics, unwanted words, and handling various common variations in Arabic.\n",
        "    3. Transforms Arabic numbers to English numbers.\n",
        "\n",
        "    Args:\n",
        "        dataset (pandas.DataFrame): The dataset containing seller item names to be processed.\n",
        "        master_file (pandas.DataFrame): A master file containing product names in English and their corresponding Arabic translations.\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: The dataset with a new column 'processed_seller_item_name' containing the processed Arabic names.\n",
        "    \"\"\"\n",
        "    # Arabic number conversion dictionary\n",
        "    arabic_to_english_numbers = str.maketrans(\"٠١٢٣٤٥٦٧٨٩\", \"0123456789\")\n",
        "\n",
        "    # Create a dictionary for English-to-Arabic translation from the Master File\n",
        "    translation_dict = dict(zip(master_file[\"product_name\"].astype(str).str.lower(), master_file[\"product_name_ar\"].astype(str)))\n",
        "\n",
        "    # List of English product names from the Master File for fuzzy matching\n",
        "    master_names_en = list(translation_dict.keys())\n",
        "\n",
        "    # Function to check if text contains English characters\n",
        "    def contains_english(text):\n",
        "        return bool(re.search(\"[A-Za-z]\", text))\n",
        "\n",
        "    # Function to translate English to Arabic (with retry logic)\n",
        "    def translate_to_arabic(text):\n",
        "        if contains_english(text):   # Only translate if it contains English\n",
        "            text_lower = text.lower().strip()\n",
        "            text = text.replace('.', ' ')\n",
        "\n",
        "            # Find the closest match from the Master File\n",
        "            match, score, _ = process.extractOne(text_lower, master_names_en, scorer=fuzz.ratio)\n",
        "\n",
        "            # If match is strong (90% similarity or higher), use the Master File translation\n",
        "            if score >= 50:\n",
        "                return translation_dict[match]\n",
        "\n",
        "            try:\n",
        "                translated_text = GoogleTranslator(source=\"english\", target=\"arabic\").translate(text_lower)\n",
        "                if translated_text and not contains_english(translated_text):\n",
        "                    return translated_text\n",
        "                else:\n",
        "                    print(f\"Translation did not return Arabic text for: ({text}) and instead return ({translation_dict[match]}) and score : {score}\")\n",
        "                    return text  # Return original if translation is not in Arabic\n",
        "            except Exception as e:\n",
        "                print(f\"Translation failed for {text}: {e}\")\n",
        "        return text  # Return original if translation fails\n",
        "\n",
        "    # Function to remove diacritics and normalize Arabic text\n",
        "    def remove_diacritics(text):\n",
        "        arabic_diacritics = re.compile(\"[\\u064B-\\u0652]\")\n",
        "        return re.sub(arabic_diacritics, \"\", text)\n",
        "\n",
        "    # Function to remove specific unwanted words (\"جديد\" with variations & \"سعر\")\n",
        "    def remove_unwanted_words(text):\n",
        "        text = re.sub(r\"جدي+د\", \"\", text)  # Remove \"جديد\" with varying \"ي\" count\n",
        "        text = re.sub(r\"\\bسعر\\b\", \"\", text)  # Remove exact match \"سعر\"\n",
        "        text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n",
        "        return text\n",
        "\n",
        "    def normalize_arabic(text):\n",
        "        text = str(text).strip()\n",
        "        text = remove_diacritics(text)\n",
        "        text = text.replace(\"أ\", \"ا\").replace(\"إ\", \"ا\").replace(\"آ\", \"ا\")  # Normalize Alef\n",
        "        text = text.replace(\"ى\", \"ي\").replace(\"ة\", \"ه\").replace(\"ٱ\", \"ا\")  # Normalize common variations\n",
        "        text = text.replace(\"ؤ\", \"و\").replace(\"ئ\", \"ي\")  # Normalize more variations\n",
        "        text = re.sub(r\"[^\\u0600-\\u06FF0-9 %\\\\/]\", \"\", text)  # Remove non-Arabic characters except numbers\n",
        "        text = text.translate(arabic_to_english_numbers)  # Convert Arabic numbers to English\n",
        "        text = re.sub(r\"(\\d+)\", r\" \\1 \", text).strip()  # Add spaces before and after numbers\n",
        "        text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n",
        "        text = re.sub(r\"ـ+\", \"\", text)  # Remove extensions in words\n",
        "        text = remove_unwanted_words(text)  # Remove \"جديد\" variations and \"سعر\"\n",
        "        return text\n",
        "\n",
        "    # Translate only English seller names to Arabic\n",
        "    dataset[\"processed_seller_item_name\"] = dataset[\"seller_item_name\"].astype(str).apply(translate_to_arabic)\n",
        "\n",
        "    # Normalize translated Arabic names\n",
        "    dataset[\"processed_seller_item_name\"] = dataset[\"processed_seller_item_name\"].apply(normalize_arabic)\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UamBnaNawnU2"
      },
      "source": [
        "### **process_file Function Documentation**\n",
        "\n",
        "**Overview**:\n",
        "The `process_file` function processes an input Excel file containing a dataset and a master file. It performs translation, similarity matching, and SKU mapping, then saves the results to an output Excel file.\n",
        "\n",
        "**Functionality**:\n",
        "This function follows these key steps:\n",
        "1. **Load Data:** Reads the input Excel file containing two sheets: `\"Dataset\"` and `\"Master File\"`.\n",
        "2. **Validate Columns:** Ensures that the necessary columns exist in both sheets.\n",
        "3. **Preprocessing:** Applies the provided preprocessing function to clean and normalize seller item names.\n",
        "4. **Compute Similarity:** Uses a machine learning model to predict the most relevant marketplace product name and compute a similarity score.\n",
        "5. **Find SKU:** Uses fuzzy matching to identify the best SKU from the master file.\n",
        "6. **Save Output:** Writes the processed dataset back to an Excel file with similarity scores and matched SKUs.\n",
        "\n",
        "**Output**:\n",
        "The processed dataset includes:\n",
        "- `marketplace_product_name_ar`: Predicted marketplace name.\n",
        "- `similarity`: Cosine similarity score.\n",
        "- `confidence`: Confidence level of the match.\n",
        "- `sku`: Mapped SKU based on fuzzy matching.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "bMV-wB71wnU2"
      },
      "outputs": [],
      "source": [
        "def process_file(input_file, master_file, model, vectorizer, preprocess_func, output_file=\"output.xlsx\"):\n",
        "    \"\"\"\n",
        "    Processes an input Excel file containing a dataset and a master file, performs translation and similarity matching,\n",
        "    and saves the results to an output Excel file.\n",
        "\n",
        "    This function performs the following steps:\n",
        "    1. Loads the input Excel file with two sheets: \"Dataset\" and \"Master File\".\n",
        "    2. Ensures that the required columns are present in the dataset and master file.\n",
        "    3. Preprocesses the seller item names using the provided preprocessing function.\n",
        "    4. Computes similarity scores between seller item names and predicted marketplace names.\n",
        "    5. Uses fuzzy matching to find the best SKU from the master file.\n",
        "    6. Saves the processed dataset with similarity and SKU information to an output Excel file.\n",
        "\n",
        "    Args:\n",
        "        input_file (str): The input Excel file containing two sheets (\"Dataset\" and \"Master File\").\n",
        "        model (sklearn.model): The trained machine learning model used for marketplace name predictions.\n",
        "        vectorizer (sklearn.feature_extraction.text.TfidfVectorizer): The TF-IDF vectorizer used for feature extraction.\n",
        "        preprocess_func (function): The preprocessing function used to process seller item names.\n",
        "        output_file (str, optional): The name of the output Excel file. Default is \"output.xlsx\".\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "\n",
        "    # Load the Excel files\n",
        "    df = pd.read_excel(input_file)  # Replace with the actual file path\n",
        "    master_df = pd.read_excel(master_file)\n",
        "\n",
        "    # Ensure seller_item_name exists in input_file\n",
        "    if 'seller_item_name' not in df.columns:\n",
        "        raise ValueError(f\"Column 'seller_item_name' not found in dataset sheet.\")\n",
        "\n",
        "    # Ensure product_name_ar exists in master_file\n",
        "    if 'product_name_ar' not in master_df.columns:\n",
        "        raise ValueError(\"Column 'product_name_ar' not found in master_file sheet.\")\n",
        "\n",
        "    # Preprocess seller item names\n",
        "    df = preprocess_func(df,master_df)\n",
        "\n",
        "    # Function to compute similarity (based on your function)\n",
        "    def compute_similarity(seller_name):\n",
        "\n",
        "        # Transform seller name to TF-IDF vector\n",
        "        seller_vector = vectorizer.transform([seller_name])\n",
        "\n",
        "        # Predict the most likely marketplace name using the TF-IDF vector\n",
        "        predicted_name = model.predict(seller_vector)[0]\n",
        "\n",
        "        # Transform predicted name to TF-IDF vector\n",
        "        predicted_vector = vectorizer.transform([predicted_name])\n",
        "\n",
        "        # Compute cosine similarity\n",
        "        similarity_score = cosine_similarity(seller_vector, predicted_vector)[0, 0]\n",
        "\n",
        "        # Set confidence levels\n",
        "        if similarity_score < 0.2:\n",
        "            matched_name = \"Not Found\"\n",
        "            confidence = \"Unknown\"\n",
        "        else:\n",
        "            matched_name = predicted_name\n",
        "            confidence = \"High\" if similarity_score > 0.8 else \"Medium\" if similarity_score > 0.6 else \"Low\"\n",
        "\n",
        "\n",
        "        return matched_name, similarity_score, confidence\n",
        "\n",
        "    # Apply matching function\n",
        "    df[['marketplace_product_name_ar', 'similarity', 'confidence']] = df['processed_seller_item_name'].apply(\n",
        "        lambda name: pd.Series(compute_similarity(name))\n",
        "    )\n",
        "\n",
        "    # Use fuzzy matching to find the best SKU\n",
        "    product_names = master_df['product_name_ar'].tolist()\n",
        "    sku_dict = dict(zip(master_df['product_name_ar'], master_df['sku']))\n",
        "\n",
        "    def find_best_sku(marketplace_name):\n",
        "        if marketplace_name == \"Not Found\":\n",
        "            return \"Not Found\"\n",
        "        match, score, _ = process.extractOne(marketplace_name, product_names, scorer=fuzz.token_sort_ratio)\n",
        "        return sku_dict.get(match, \"Not Found\") if score > 70 else \"Not Found\"\n",
        "\n",
        "    df['sku'] = df['marketplace_product_name_ar'].apply(find_best_sku)\n",
        "\n",
        "    # Drop duplicate columns & temp column\n",
        "    df.drop(columns=['processed_seller_item_name'], inplace=True)\n",
        "\n",
        "    # Save df to an Excel file\n",
        "    df.to_excel(\"output.xlsx\", sheet_name=\"dataset\", index=False)\n",
        "\n",
        "    print(f\"Processed file saved as {output_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd6_5PhwwnU3"
      },
      "source": [
        "### **Execution Time Analysis for process_file Function**\n",
        "\n",
        "measures the execution time of the `process_file` function while processing an Excel file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2FgsC9awnU3",
        "outputId": "46756775-b1f2-4a0e-c3a5-c4feb8d9ac63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed file saved as output.xlsx\n",
            "Total Execution Time: 71695.02 ms\n",
            "Average Time per Sample: 71.70 ms\n"
          ]
        }
      ],
      "source": [
        "start_time = time()\n",
        "\n",
        "# Process the test file\n",
        "process_file(\"test file.xlsx\", 'master_file.xlsx', model, vectorizer, preprocess_function, output_file=\"output.xlsx\") # edit the path if you want to test on another test data\n",
        "\n",
        "# Calculate total execution time in milliseconds\n",
        "execution_time = (time() - start_time) * 1000\n",
        "\n",
        "# Get the actual number of samples processed\n",
        "num_samples = pd.read_excel(\"test file.xlsx\", sheet_name=\"Dataset\").shape[0]\n",
        "\n",
        "# Calculate average time per sample dynamically\n",
        "average_time_per_sample = execution_time / num_samples if num_samples > 0 else 0\n",
        "\n",
        "# Print execution results\n",
        "print(f\"Total Execution Time: {round(execution_time, 2)} ms\")\n",
        "print(f\"Average Time per Sample: {average_time_per_sample:.2f} ms\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}