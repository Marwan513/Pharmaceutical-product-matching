{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Required Libraries\n",
    "\n",
    "This cell imports various Python libraries needed for data processing, machine learning, and text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from time import time\n",
    "from rapidfuzz import process, fuzz\n",
    "from deep_translator import GoogleTranslator\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Trained Model and Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and vectorizer loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load the model and vectorizer from the 'models' directory\n",
    "model = joblib.load(\"models/logistic_regression_model.pkl\")\n",
    "vectorizer = joblib.load(\"models/tfidf_vectorizer.pkl\")\n",
    "\n",
    "print(\"Model and vectorizer loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Preprocesses the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(dataset, master_file):\n",
    "    \"\"\"\n",
    "    Preprocesses the dataset by translating English seller item names to Arabic and normalizing the Arabic text.\n",
    "\n",
    "    This function performs the following:\n",
    "    1. Translates English product names to Arabic using fuzzy matching with a master file and Google Translator as a fallback.\n",
    "    2. Normalizes the Arabic text by removing diacritics, unwanted words, and handling various common variations in Arabic.\n",
    "    3. Transforms Arabic numbers to English numbers.\n",
    "\n",
    "    Args:\n",
    "        dataset (pandas.DataFrame): The dataset containing seller item names to be processed.\n",
    "        master_file (pandas.DataFrame): A master file containing product names in English and their corresponding Arabic translations.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The dataset with a new column 'processed_seller_item_name' containing the processed Arabic names.\n",
    "    \"\"\"\n",
    "    # Arabic number conversion dictionary\n",
    "    arabic_to_english_numbers = str.maketrans(\"٠١٢٣٤٥٦٧٨٩\", \"0123456789\")\n",
    "\n",
    "    # Create a dictionary for English-to-Arabic translation from the Master File\n",
    "    translation_dict = dict(zip(master_file[\"product_name\"].astype(str).str.lower(), master_file[\"product_name_ar\"].astype(str)))\n",
    "\n",
    "    # List of English product names from the Master File for fuzzy matching\n",
    "    master_names_en = list(translation_dict.keys())\n",
    "\n",
    "    # Function to check if text contains English characters\n",
    "    def contains_english(text):\n",
    "        return bool(re.search(\"[A-Za-z]\", text))\n",
    "\n",
    "    # Function to translate English to Arabic (with retry logic)\n",
    "    def translate_to_arabic(text):\n",
    "        if contains_english(text):   # Only translate if it contains English\n",
    "            text_lower = text.lower().strip()\n",
    "            text = text.replace('.', ' ')\n",
    "\n",
    "            # Find the closest match from the Master File\n",
    "            match, score, _ = process.extractOne(text_lower, master_names_en, scorer=fuzz.ratio)\n",
    "\n",
    "            # If match is strong (90% similarity or higher), use the Master File translation\n",
    "            if score >= 50:\n",
    "                return translation_dict[match]\n",
    "\n",
    "            try:\n",
    "                translated_text = GoogleTranslator(source=\"english\", target=\"arabic\").translate(text_lower)\n",
    "                if translated_text and not contains_english(translated_text):\n",
    "                    return translated_text\n",
    "                else:\n",
    "                    print(f\"Translation did not return Arabic text for: ({text}) and instead return ({translation_dict[match]}) and score : {score}\")\n",
    "                    return text  # Return original if translation is not in Arabic\n",
    "            except Exception as e:\n",
    "                print(f\"Translation failed for {text}: {e}\")\n",
    "        return text  # Return original if translation fails\n",
    "\n",
    "    # Function to remove diacritics and normalize Arabic text\n",
    "    def remove_diacritics(text):\n",
    "        arabic_diacritics = re.compile(\"[\\u064B-\\u0652]\")\n",
    "        return re.sub(arabic_diacritics, \"\", text)\n",
    "\n",
    "    # Function to remove specific unwanted words (\"جديد\" with variations & \"سعر\")\n",
    "    def remove_unwanted_words(text):\n",
    "        text = re.sub(r\"جدي+د\", \"\", text)  # Remove \"جديد\" with varying \"ي\" count\n",
    "        text = re.sub(r\"\\bسعر\\b\", \"\", text)  # Remove exact match \"سعر\"\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n",
    "        return text\n",
    "\n",
    "    def normalize_arabic(text):\n",
    "        text = str(text).strip()\n",
    "        text = remove_diacritics(text)\n",
    "        text = text.replace(\"أ\", \"ا\").replace(\"إ\", \"ا\").replace(\"آ\", \"ا\")  # Normalize Alef\n",
    "        text = text.replace(\"ى\", \"ي\").replace(\"ة\", \"ه\").replace(\"ٱ\", \"ا\")  # Normalize common variations\n",
    "        text = text.replace(\"ؤ\", \"و\").replace(\"ئ\", \"ي\")  # Normalize more variations\n",
    "        text = re.sub(r\"[^\\u0600-\\u06FF0-9 %\\\\/]\", \"\", text)  # Remove non-Arabic characters except numbers\n",
    "        text = text.translate(arabic_to_english_numbers)  # Convert Arabic numbers to English\n",
    "        text = re.sub(r\"(\\d+)\", r\" \\1 \", text).strip()  # Add spaces before and after numbers\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n",
    "        text = re.sub(r\"ـ+\", \"\", text)  # Remove extensions in words\n",
    "        text = remove_unwanted_words(text)  # Remove \"جديد\" variations and \"سعر\"\n",
    "        return text\n",
    "\n",
    "    # Translate only English seller names to Arabic\n",
    "    dataset[\"processed_seller_item_name\"] = dataset[\"seller_item_name\"].astype(str).apply(translate_to_arabic)\n",
    "\n",
    "    # Normalize translated Arabic names\n",
    "    dataset[\"processed_seller_item_name\"] = dataset[\"processed_seller_item_name\"].apply(normalize_arabic)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **process_file Function Documentation**\n",
    "\n",
    "**Overview**:\n",
    "The `process_file` function processes an input Excel file containing a dataset and a master file. It performs translation, similarity matching, and SKU mapping, then saves the results to an output Excel file.\n",
    "\n",
    "**Functionality**:\n",
    "This function follows these key steps:\n",
    "1. **Load Data:** Reads the input Excel file containing two sheets: `\"Dataset\"` and `\"Master File\"`.\n",
    "2. **Validate Columns:** Ensures that the necessary columns exist in both sheets.\n",
    "3. **Preprocessing:** Applies the provided preprocessing function to clean and normalize seller item names.\n",
    "4. **Compute Similarity:** Uses a machine learning model to predict the most relevant marketplace product name and compute a similarity score.\n",
    "5. **Find SKU:** Uses fuzzy matching to identify the best SKU from the master file.\n",
    "6. **Save Output:** Writes the processed dataset back to an Excel file with similarity scores and matched SKUs.\n",
    "\n",
    "**Output**:\n",
    "The processed dataset includes:\n",
    "- `marketplace_product_name_ar`: Predicted marketplace name.\n",
    "- `similarity`: Cosine similarity score.\n",
    "- `confidence`: Confidence level of the match.\n",
    "- `sku`: Mapped SKU based on fuzzy matching.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(input_file, model, vectorizer, preprocess_func, output_file=\"output.xlsx\"):\n",
    "    \"\"\"\n",
    "    Processes an input Excel file containing a dataset and a master file, performs translation and similarity matching, \n",
    "    and saves the results to an output Excel file.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Loads the input Excel file with two sheets: \"Dataset\" and \"Master File\".\n",
    "    2. Ensures that the required columns are present in the dataset and master file.\n",
    "    3. Preprocesses the seller item names using the provided preprocessing function.\n",
    "    4. Computes similarity scores between seller item names and predicted marketplace names.\n",
    "    5. Uses fuzzy matching to find the best SKU from the master file.\n",
    "    6. Saves the processed dataset with similarity and SKU information to an output Excel file.\n",
    "\n",
    "    Args:\n",
    "        input_file (str): The input Excel file containing two sheets (\"Dataset\" and \"Master File\").\n",
    "        model (sklearn.model): The trained machine learning model used for marketplace name predictions.\n",
    "        vectorizer (sklearn.feature_extraction.text.TfidfVectorizer): The TF-IDF vectorizer used for feature extraction.\n",
    "        preprocess_func (function): The preprocessing function used to process seller item names.\n",
    "        output_file (str, optional): The name of the output Excel file. Default is \"output.xlsx\".\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the Excel file (both sheets)\n",
    "    sheets = pd.read_excel(input_file, sheet_name=[\"Dataset\", \"Master File\"])\n",
    "    df = sheets[\"Dataset\"]\n",
    "    master_df = sheets[\"Master File\"]\n",
    "\n",
    "    # Ensure necessary columns exist in dataset\n",
    "    required_columns = ['seller_item_name', 'marketplace_product_name_ar', 'similarity', 'confidence', 'sku']\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Column '{col}' not found in dataset sheet.\")\n",
    "    \n",
    "    # Ensure product_name_ar exists in master_file\n",
    "    if 'product_name_ar' not in master_df.columns:\n",
    "        raise ValueError(\"Column 'product_name_ar' not found in master_file sheet.\")\n",
    "    \n",
    "    # Preprocess seller item names\n",
    "    df = preprocess_func(df,master_df)\n",
    "\n",
    "    # Function to compute similarity (based on your function)\n",
    "    def compute_similarity(seller_name):\n",
    "\n",
    "        # Transform seller name to TF-IDF vector\n",
    "        seller_vector = vectorizer.transform([seller_name])\n",
    "\n",
    "        # Predict the most likely marketplace name using the TF-IDF vector\n",
    "        predicted_name = model.predict(seller_vector)[0]\n",
    "\n",
    "        # Transform predicted name to TF-IDF vector\n",
    "        predicted_vector = vectorizer.transform([predicted_name])\n",
    "\n",
    "        # Compute cosine similarity\n",
    "        similarity_score = cosine_similarity(seller_vector, predicted_vector)[0, 0]\n",
    "\n",
    "        # Set confidence levels\n",
    "        if similarity_score < 0.2:\n",
    "            matched_name = \"Not Found\"\n",
    "            confidence = \"Unknown\"\n",
    "        else:\n",
    "            matched_name = predicted_name\n",
    "            confidence = \"High\" if similarity_score > 0.8 else \"Medium\" if similarity_score > 0.6 else \"Low\"\n",
    "\n",
    "\n",
    "        return matched_name, similarity_score, confidence\n",
    "\n",
    "    # Apply matching function\n",
    "    df[['marketplace_product_name_ar', 'similarity', 'confidence']] = df['processed_seller_item_name'].apply(\n",
    "        lambda name: pd.Series(compute_similarity(name))\n",
    "    )\n",
    "\n",
    "    # Use fuzzy matching to find the best SKU\n",
    "    product_names = master_df['product_name_ar'].tolist()\n",
    "    sku_dict = dict(zip(master_df['product_name_ar'], master_df['sku']))\n",
    "\n",
    "    def find_best_sku(marketplace_name):\n",
    "        if marketplace_name == \"Not Found\":\n",
    "            return \"Not Found\"\n",
    "        match, score, _ = process.extractOne(marketplace_name, product_names, scorer=fuzz.token_sort_ratio)\n",
    "        return sku_dict.get(match, \"Not Found\") if score > 70 else \"Not Found\"\n",
    "\n",
    "    df['sku'] = df['marketplace_product_name_ar'].apply(find_best_sku)\n",
    "\n",
    "    # Drop duplicate columns & temp column\n",
    "    df.drop(columns=['processed_seller_item_name'], inplace=True)\n",
    "\n",
    "    # Save the updated file\n",
    "    with pd.ExcelWriter(output_file) as writer:\n",
    "        df.to_excel(writer, sheet_name=\"dataset\", index=False)\n",
    "        master_df.to_excel(writer, sheet_name=\"master_file\", index=False)\n",
    "    \n",
    "    print(f\"Processed file saved as {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Execution Time Analysis for process_file Function**\n",
    "\n",
    "measures the execution time of the `process_file` function while processing an Excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed file saved as output.xlsx\n",
      "Total Execution Time: 85556.72 ms\n",
      "Average Time per Sample: 85.56 ms\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "\n",
    "# Process the test file\n",
    "process_file(\"test file.xlsx\", model, vectorizer, preprocess_function, output_file=\"output.xlsx\") # edit the path if you want to test on another test data\n",
    "\n",
    "# Calculate total execution time in milliseconds\n",
    "execution_time = (time() - start_time) * 1000  \n",
    "\n",
    "# Get the actual number of samples processed\n",
    "num_samples = pd.read_excel(\"test file.xlsx\", sheet_name=\"Dataset\").shape[0]\n",
    "\n",
    "# Calculate average time per sample dynamically\n",
    "average_time_per_sample = execution_time / num_samples if num_samples > 0 else 0\n",
    "\n",
    "# Print execution results\n",
    "print(f\"Total Execution Time: {round(execution_time, 2)} ms\")\n",
    "print(f\"Average Time per Sample: {average_time_per_sample:.2f} ms\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
